{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, merge\n",
    "from keras.layers import Reshape,LeakyReLU,ZeroPadding2D\n",
    "from keras.layers.core import Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Deconvolution2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.objectives import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "#from tqdm import tqdm\n",
    "import scipy.misc as im\n",
    "#K.set_image_dim_ordering('th') \n",
    "IN_CH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob, pickle\n",
    "import os, sys\n",
    "import argparse\n",
    "#import cv2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, merge\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Deconvolution2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import math\n",
    "K.set_image_dim_ordering('tf') \n",
    "\n",
    "img_rows = 256\n",
    "img_cols = 256\n",
    "SHAPE = 256\n",
    "BATCH = 4\n",
    "IN_CH = 3\n",
    "OUT_CH = 3\n",
    "LAMBDA = 100\n",
    "NF = 64 # number of filter\n",
    "BATCH_SIZE = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_input(img,mode='AtoB'):\n",
    "    \"\"\"\n",
    "    img: an 512x256x3 image\n",
    "    :return: [input, output]\n",
    "    \"\"\"\n",
    "    input, output = img[:,:img_cols,:], img[:,img_cols:,:]\n",
    "\n",
    "    if mode == 'BtoA':\n",
    "        input, output = output, input\n",
    "    return [input, output]\n",
    "\n",
    "def get_data(datadir):\n",
    "    #datadir = args.data\n",
    "    # assume each image is 512x256 split to left and right\n",
    "    imgs = glob.glob(os.path.join(datadir, '*.jpg'))\n",
    "    data_X = np.zeros((len(imgs),img_rows,img_cols,3))\n",
    "    data_Y = np.zeros((len(imgs),img_rows,img_cols,3))\n",
    "    i = 0\n",
    "    for file in imgs:\n",
    "        #img = cv2.imread(file,cv2.IMREAD_COLOR)\n",
    "        img=Image.open(file)\n",
    "        img = img.resize((img_cols*2, img_rows), Image.LANCZOS)\n",
    "        \n",
    "        img = np.array(img)\n",
    "        #img = cv2.resize(img, (img_cols*2, img_rows)) \n",
    "        #print('{} {},{}'.format(i,np.shape(img)[0],np.shape(img)[1]))\n",
    "        \n",
    "\n",
    "        X, Y = split_input(img)\n",
    "\n",
    "        data_X[i,:,:,:] = X\n",
    "        data_Y[i,:,:,:] = Y\n",
    "        i = i+1\n",
    "    return data_X, data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x,data_y=get_data('train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "#cv2.imshow(data_x[0])\n",
    "#cv2.waitKey(0)\n",
    "print(type(data_x))\n",
    "print(data_x.shape)\n",
    "#img=data_x[0,:,:,:]\n",
    "#cv2.imshow(img,'img')\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    \n",
    "    input_tensor = Input(shape=(img_rows,img_cols,IN_CH)) # type: Input\n",
    "    output_ch = 3\n",
    "    filters=128\n",
    "    x =                       Conv2D(         filters*1, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( input_tensor )       ; e1 = x\n",
    "    x = BatchNormalization()( Conv2D(         filters*2, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) ) ); e2 = x\n",
    "    x = BatchNormalization()( Conv2D(         filters*4, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) ) ); e3 = x\n",
    "    x = BatchNormalization()( Conv2D(         filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) ) ); e4 = x\n",
    "    x = BatchNormalization()( Conv2D(         filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) ) ); e5 = x\n",
    "    x = BatchNormalization()( Conv2D(         filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) ) ); e6 = x\n",
    "    x = BatchNormalization()( Conv2D(         filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) ) ); e7 = x\n",
    "    x =                       Conv2D(         filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) )  ; e8 = x\n",
    "    # dec\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([Dropout(0.5)(x), e7])\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([Dropout(0.5)(x), e6])\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([Dropout(0.5)(x), e5])\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([x, e4])\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*4, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([x, e3])\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*2, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([x, e2])\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*1, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([x, e1])\n",
    "    x =                       Conv2DTranspose(output_ch, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) )\n",
    "    \n",
    "    x = Activation(\"tanh\")(x)\n",
    "    \n",
    "    unet = Model(inputs=input_tensor, outputs=x)\n",
    "    \n",
    "    return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 256, 256, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 128, 128, 128) 6272        input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)        (None, 128, 128, 128) 0           conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 64, 64, 256)   524544      leaky_re_lu_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 64, 64, 256)   1024        conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)        (None, 64, 64, 256)   0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 32, 32, 512)   2097664     leaky_re_lu_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 32, 32, 512)   2048        conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)        (None, 32, 32, 512)   0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 16, 16, 1024)  8389632     leaky_re_lu_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 16, 16, 1024)  4096        conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)        (None, 16, 16, 1024)  0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 8, 8, 1024)    16778240    leaky_re_lu_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 8, 8, 1024)    4096        conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)        (None, 8, 8, 1024)    0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 4, 4, 1024)    16778240    leaky_re_lu_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 4, 4, 1024)    4096        conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)        (None, 4, 4, 1024)    0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 2, 2, 1024)    16778240    leaky_re_lu_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 2, 2, 1024)    4096        conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)        (None, 2, 2, 1024)    0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 1, 1, 1024)    16778240    leaky_re_lu_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 1, 1, 1024)    0           conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTransp (None, 2, 2, 1024)    16778240    activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 2, 2, 1024)    4096        conv2d_transpose_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 2, 2, 1024)    0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 2, 2, 2048)    0           dropout_1[0][0]                  \n",
      "                                                                   batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 2, 2, 2048)    0           concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTransp (None, 4, 4, 1024)    33555456    activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 4, 4, 1024)    4096        conv2d_transpose_2[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 4, 4, 1024)    0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 4, 4, 2048)    0           dropout_2[0][0]                  \n",
      "                                                                   batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 4, 4, 2048)    0           concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTransp (None, 8, 8, 1024)    33555456    activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 8, 8, 1024)    4096        conv2d_transpose_3[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 8, 8, 1024)    0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 8, 8, 2048)    0           dropout_3[0][0]                  \n",
      "                                                                   batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 8, 8, 2048)    0           concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTransp (None, 16, 16, 1024)  33555456    activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 16, 16, 1024)  4096        conv2d_transpose_4[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 16, 16, 2048)  0           batch_normalization_10[0][0]     \n",
      "                                                                   batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 16, 16, 2048)  0           concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTransp (None, 32, 32, 512)   16777728    activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 32, 32, 512)   2048        conv2d_transpose_5[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 32, 32, 1024)  0           batch_normalization_11[0][0]     \n",
      "                                                                   batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 32, 32, 1024)  0           concatenate_5[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTransp (None, 64, 64, 256)   4194560     activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 64, 64, 256)   1024        conv2d_transpose_6[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)      (None, 64, 64, 512)   0           batch_normalization_12[0][0]     \n",
      "                                                                   batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 64, 64, 512)   0           concatenate_6[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTransp (None, 128, 128, 128) 1048704     activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 128, 128, 128) 512         conv2d_transpose_7[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)      (None, 128, 128, 256) 0           batch_normalization_13[0][0]     \n",
      "                                                                   conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 128, 128, 256) 0           concatenate_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTransp (None, 256, 256, 3)   12291       activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 256, 256, 3)   0           conv2d_transpose_8[0][0]         \n",
      "====================================================================================================\n",
      "Total params: 217,648,387\n",
      "Trainable params: 217,628,675\n",
      "Non-trainable params: 19,712\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gx=generator_model()\n",
    "gx.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_model():    \n",
    "    inputs = Input(shape=(img_cols,img_rows,IN_CH*2))\n",
    "    d = ZeroPadding2D(padding=(1,1))(inputs)\n",
    "    d = Conv2D(64, kernel_size=(4, 4), strides=(2, 2))(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = ZeroPadding2D(padding=(1,1))(d)\n",
    "    d = Conv2D(128, kernel_size=(4, 4), strides=(2, 2))(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = ZeroPadding2D(padding=(1,1))(d)\n",
    "    d = Conv2D(256, kernel_size=(4, 4), strides=(2, 2))(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = ZeroPadding2D(padding=(1,1))(d)\n",
    "    d = Convolution2D(512, kernel_size=(4, 4), strides=(1, 1))(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = ZeroPadding2D(padding=(1,1))(d)\n",
    "    d = Convolution2D(1, kernel_size=(4, 4), strides=(1, 1),activation='sigmoid')(d)\n",
    "    model = Model(inputs,d)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 256, 256, 6)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 258, 258, 6)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 128, 128, 64)      6208      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 130, 130, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 64, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 66, 66, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 34, 34, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 31, 31, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 31, 31, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 33, 33, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 30, 30, 1)         8193      \n",
      "=================================================================\n",
      "Total params: 2,767,809\n",
      "Trainable params: 2,767,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dx=discriminator_model()\n",
    "dx.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_containing_discriminator(generator, discriminator):\n",
    "    inputs = Input((img_cols, img_rows,IN_CH))\n",
    "    x_generator = generator(inputs)\n",
    "    merged=Concatenate()([inputs, x_generator])\n",
    "    #merged = merge([inputs, x_generator], mode='concat',concat_axis=-1)\n",
    "    discriminator.trainable = False\n",
    "    x_discriminator = discriminator(merged)\n",
    "    \n",
    "    model = Model(inputs,[x_generator,x_discriminator])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 256, 256, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 256, 256, 3)   217648387   input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)      (None, 256, 256, 6)   0           input_3[0][0]                    \n",
      "                                                                   model_1[1][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "model_2 (Model)                  (None, 30, 30, 1)     2767809     concatenate_8[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 220,416,196\n",
      "Trainable params: 217,628,675\n",
      "Non-trainable params: 2,787,521\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gcd=generator_containing_discriminator(gx,dx)\n",
    "gcd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_on_generator_loss(y_true,y_pred):\n",
    "    return K.mean(K.binary_crossentropy(y_pred,y_true), axis=(1,2,3))\n",
    "\n",
    "def generator_l1_loss(y_true,y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true),axis=(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_pic(generator,target,e):\n",
    "    pic = generator.predict(target)\n",
    "    pic = np.squeeze(pic,axis=0)\n",
    "    target = np.squeeze(target,axis=0)\n",
    "    im.imsave('target_%d.png' % e,target)\n",
    "    im.imsave('pic_%d.png' % e,pic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epochs,batchsize):\n",
    "    \n",
    "    \n",
    "    \n",
    "    pic,target=get_data('train')\n",
    "    pic = pic.astype('float32')\n",
    "    target = target.astype('float32')\n",
    "    pic = (pic - 127.5) / 127.5\n",
    "    target = (target - 127.5) / 127.5\n",
    "    batchCount = int(pic.shape[0] / batchsize)\n",
    "    print('Epochs  ',epochs)\n",
    "    print('Batch_size   ',batchsize)\n",
    "    print('Batches per epoch    ',batchCount)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    generator = generator_model()\n",
    "    \n",
    "    discriminator = discriminator_model()\n",
    "    \n",
    "    gan = generator_containing_discriminator(generator,discriminator)\n",
    "    \n",
    "    generator.compile(loss=generator_l1_loss, optimizer='RMSprop')\n",
    "    \n",
    "    gan.compile(loss=[generator_l1_loss,discriminator_on_generator_loss] , optimizer='RMSprop')\n",
    "    \n",
    "    discriminator.trainable = True\n",
    "    \n",
    "    discriminator.compile(loss=discriminator_on_generator_loss, optimizer='RMSprop')\n",
    "    \n",
    "    \n",
    "    G_loss = []\n",
    "    D_loss = []\n",
    "    for e in range(1,epochs+1):\n",
    "        print('Epoch       {}',e)\n",
    "        for nums in range(batchCount):\n",
    "            random_number = np.random.randint(1,pic.shape[0],size=batchsize)\n",
    "            batch_pic = pic[random_number]\n",
    "            batch_target = target[random_number]\n",
    "            batch_target2 = np.tile(batch_target,(2,1,1,1))\n",
    "            y_dis = np.zeros((2*batchsize,30,30,1))\n",
    "            y_dis[:batchsize] = 1.0\n",
    "            print('done')\n",
    "            ''' \n",
    "            generated_pic = generator.predict(batch_target)\n",
    "            #Default is concat first dimention\n",
    "            concat_pic = np.concatenate((batch_pic,generated_pic))\n",
    "            \n",
    "            dis_input = np.concatenate((concat_pic,batch_target2),axis=-1)\n",
    "            \n",
    "            dloss = discriminator.train_on_batch(dis_input,y_dis)\n",
    "            \n",
    "            random_number = np.random.randint(1,pic.shape[0],size=batchsize)\n",
    "            \n",
    "            train_target = target[random_number]\n",
    "            \n",
    "            batch_pic = pic[random_number]\n",
    "            \n",
    "            y_gener = np.ones((batchsize,30,30,1))\n",
    "            \n",
    "            discriminator.trainable = False\n",
    "            \n",
    "            gloss = gan.train_on_batch(train_target,[batch_pic,y_gener])\n",
    "            \n",
    "            discriminator.trainable = True\n",
    "            \n",
    "        G_loss.append(gloss)\n",
    "        D_loss.append(dloss)\n",
    "        if e % 50 == 0 or e == 1:\n",
    "            generate_pic(generator,target[0:1],e)\n",
    "            \n",
    "            generator.save('Model_para/pix2pix_g_epoch_%d.h5' % e)\n",
    "            \n",
    "            discriminator.save('Model_para/pix2pix_d_epoch_%d.h5' % e)\n",
    "            \n",
    "            gan.save('Model_para/pix2pix_gan_epoch_%d.h5' % e)\n",
    "            \n",
    "    D_loss = np.array(D_loss)\n",
    "    G_loss = np.array(G_loss)\n",
    "    np.save('Model_para/dloss.npy',D_loss)\n",
    "    np.save('Model_para/gloss.npy',G_loss)\n",
    "    '''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs   1\n",
      "Batch_size    10\n",
      "Batches per epoch     5\n",
      "Epoch       {} 1\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
