{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, merge\n",
    "from keras.layers import Reshape,LeakyReLU,ZeroPadding2D\n",
    "from keras.layers.core import Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Deconvolution2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.objectives import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "#from tqdm import tqdm\n",
    "import scipy.misc as im\n",
    "#K.set_image_dim_ordering('th') \n",
    "IN_CH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob, pickle\n",
    "import os, sys\n",
    "import argparse\n",
    "#import cv2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, merge\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Deconvolution2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import math\n",
    "K.set_image_dim_ordering('tf') \n",
    "\n",
    "img_rows = 256\n",
    "img_cols = 256\n",
    "SHAPE = 256\n",
    "BATCH = 4\n",
    "IN_CH = 3\n",
    "OUT_CH = 3\n",
    "LAMBDA = 100\n",
    "NF = 64 # number of filter\n",
    "BATCH_SIZE = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split_input(img,mode='AtoB'):\n",
    "    \"\"\"\n",
    "    img: an 512x256x3 image\n",
    "    :return: [input, output]\n",
    "    \"\"\"\n",
    "    input, output = img[:,:img_cols,:], img[:,img_cols:,:]\n",
    "\n",
    "    if mode == 'BtoA':\n",
    "        input, output = output, input\n",
    "    return [input, output]\n",
    "\n",
    "def get_data(datadir):\n",
    "    #datadir = args.data\n",
    "    # assume each image is 512x256 split to left and right\n",
    "    imgs = glob.glob(os.path.join(datadir, '*.jpg'))\n",
    "    data_X = np.zeros((len(imgs),img_rows,img_cols,3))\n",
    "    data_Y = np.zeros((len(imgs),img_rows,img_cols,3))\n",
    "    i = 0\n",
    "    for file in imgs:\n",
    "        #img = cv2.imread(file,cv2.IMREAD_COLOR)\n",
    "        img=Image.open(file)\n",
    "        img = img.resize((img_cols*2, img_rows), Image.LANCZOS)\n",
    "        \n",
    "        img = np.array(img)\n",
    "        #img = cv2.resize(img, (img_cols*2, img_rows)) \n",
    "        #print('{} {},{}'.format(i,np.shape(img)[0],np.shape(img)[1]))\n",
    "        \n",
    "\n",
    "        X, Y = split_input(img)\n",
    "\n",
    "        data_X[i,:,:,:] = X\n",
    "        data_Y[i,:,:,:] = Y\n",
    "        i = i+1\n",
    "    return data_X, data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_x,data_y=get_data('train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(50, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "#cv2.imshow(data_x[0])\n",
    "#cv2.waitKey(0)\n",
    "print(type(data_x))\n",
    "print(data_x.shape)\n",
    "#img=data_x[0,:,:,:]\n",
    "#cv2.imshow(img,'img')\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    \n",
    "    input_tensor = Input(shape=(img_rows,img_cols,IN_CH)) # type: Input\n",
    "    output_ch = 3\n",
    "    filters=128\n",
    "    x =                       Conv2D(         filters*1, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( input_tensor )       ; e1 = x\n",
    "    x = BatchNormalization()( Conv2D(         filters*2, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) ) ); e2 = x\n",
    "    x = BatchNormalization()( Conv2D(         filters*4, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) ) ); e3 = x\n",
    "    x = BatchNormalization()( Conv2D(         filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) ) ); e4 = x\n",
    "    x = BatchNormalization()( Conv2D(         filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) ) ); e5 = x\n",
    "    x = BatchNormalization()( Conv2D(         filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) ) ); e6 = x\n",
    "    x = BatchNormalization()( Conv2D(         filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) ) ); e7 = x\n",
    "    x =                       Conv2D(         filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( LeakyReLU(0.2)(x) )  ; e8 = x\n",
    "    # dec\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([Dropout(0.5)(x), e7])\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([Dropout(0.5)(x), e6])\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([Dropout(0.5)(x), e5])\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*8, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([x, e4])\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*4, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([x, e3])\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*2, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([x, e2])\n",
    "    x = BatchNormalization()( Conv2DTranspose(filters*1, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) ) ); x = Concatenate()([x, e1])\n",
    "    x =                       Conv2DTranspose(output_ch, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")( Activation(\"relu\")(x) )\n",
    "    \n",
    "    x = Activation(\"tanh\")(x)\n",
    "    \n",
    "    unet = Model(inputs=input_tensor, outputs=x)\n",
    "    \n",
    "    return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 128)     6272      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 256)       524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64, 64, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 1024)      8389632   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 1024)        16778240  \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 1024)        16778240  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 2, 2, 1024)        16778240  \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2, 2, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 1, 1, 1024)        16778240  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 2, 2, 1024)        16778240  \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 2, 2, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 2, 1024)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_1 (Concatenate)  (None, 2, 2, 2048)        0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 2, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 4, 4, 1024)        33555456  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_2 (Concatenate)  (None, 4, 4, 2048)        0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4, 4, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 8, 8, 1024)        33555456  \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8, 8, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 1024)        0         \n",
      "_________________________________________________________________\n",
      "concatenate_3 (Concatenate)  (None, 8, 8, 2048)        0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 2048)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 16, 16, 1024)      33555456  \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "concatenate_4 (Concatenate)  (None, 16, 16, 2048)      0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 16, 2048)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 32, 32, 512)       16777728  \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 32, 32, 512)       2048      \n",
      "_________________________________________________________________\n",
      "concatenate_5 (Concatenate)  (None, 32, 32, 1024)      0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 32, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 64, 64, 256)       4194560   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 64, 64, 256)       1024      \n",
      "_________________________________________________________________\n",
      "concatenate_6 (Concatenate)  (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 64, 64, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 128, 128, 128)     1048704   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128, 128, 128)     512       \n",
      "_________________________________________________________________\n",
      "concatenate_7 (Concatenate)  (None, 128, 128, 256)     0         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128, 128, 256)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 256, 256, 3)       12291     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256, 256, 3)       0         \n",
      "=================================================================\n",
      "Total params: 217,648,387.0\n",
      "Trainable params: 217,628,675.0\n",
      "Non-trainable params: 19,712.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gx=generator_model()\n",
    "gx.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def discriminator_model():    \n",
    "    inputs = Input(shape=(img_cols,img_rows,IN_CH*2))\n",
    "    d = ZeroPadding2D(padding=(1,1))(inputs)\n",
    "    d = Conv2D(64, kernel_size=(4, 4), strides=(2, 2))(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = ZeroPadding2D(padding=(1,1))(d)\n",
    "    d = Conv2D(128, kernel_size=(4, 4), strides=(2, 2))(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = ZeroPadding2D(padding=(1,1))(d)\n",
    "    d = Conv2D(256, kernel_size=(4, 4), strides=(2, 2))(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = ZeroPadding2D(padding=(1,1))(d)\n",
    "    d = Convolution2D(512, kernel_size=(4, 4), strides=(1, 1))(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    d = ZeroPadding2D(padding=(1,1))(d)\n",
    "    d = Convolution2D(1, kernel_size=(4, 4), strides=(1, 1),activation='sigmoid')(d)\n",
    "    model = Model(inputs,d)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 256, 256, 6)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 258, 258, 6)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 128, 128, 64)      6208      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 130, 130, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 64, 128)       131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 66, 66, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 256)       524544    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 34, 34, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 31, 31, 512)       2097664   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 31, 31, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 33, 33, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 30, 30, 1)         8193      \n",
      "=================================================================\n",
      "Total params: 2,767,809.0\n",
      "Trainable params: 2,767,809.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dx=discriminator_model()\n",
    "dx.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generator_containing_discriminator(generator, discriminator):\n",
    "    inputs = Input((img_cols, img_rows,IN_CH))\n",
    "    x_generator = generator(inputs)\n",
    "    merged=Concatenate()([inputs, x_generator])\n",
    "    #merged = merge([inputs, x_generator], mode='concat',concat_axis=-1)\n",
    "    discriminator.trainable = False\n",
    "    x_discriminator = discriminator(merged)\n",
    "    \n",
    "    model = Model(inputs,[x_generator,x_discriminator])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 256, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 256, 256, 3)       217648387 \n",
      "_________________________________________________________________\n",
      "concatenate_8 (Concatenate)  (None, 256, 256, 6)       0         \n",
      "_________________________________________________________________\n",
      "model_2 (Model)              (None, 30, 30, 1)         2767809   \n",
      "=================================================================\n",
      "Total params: 220,416,196.0\n",
      "Trainable params: 220,396,484.0\n",
      "Non-trainable params: 19,712.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gcd=generator_containing_discriminator(gx,dx)\n",
    "gcd.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def discriminator_on_generator_loss(y_true,y_pred):\n",
    "    return K.mean(K.binary_crossentropy(y_pred,y_true), axis=(1,2,3))\n",
    "\n",
    "def generator_l1_loss(y_true,y_pred):\n",
    "    return K.mean(K.abs(y_pred - y_true),axis=(1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_pic(generator,target,filename):\n",
    "    pic = generator.predict(target)\n",
    "    pic = np.squeeze(pic,axis=0)\n",
    "    #target = np.squeeze(target,axis=0)\n",
    "    #im.imsave('target_%d_%d.png' % (e,n),target)\n",
    "    #filename='pic'+str(n)+'.png'\n",
    "    #gg=os.path.join(paths, filename)\n",
    "    #print('done')\n",
    "    im.imsave(filename,pic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#pic2,target2=get_data('test')\n",
    "#e=1\n",
    "\n",
    "#generate_pic(gx,pic2[0:1],paths,3)\n",
    "#print()   target2.shape[0]\n",
    "#for i in range(target2.shape[0]):\n",
    "#   generate_pic(gx,target2[i:i+1],i+1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(epochs,batchsize):\n",
    "    \n",
    "    \n",
    "    \n",
    "    pic,target=get_data('train')\n",
    "    pic = pic.astype('float32')\n",
    "    target = target.astype('float32')\n",
    "    pic = (pic - 127.5) / 127.5\n",
    "    target = (target - 127.5) / 127.5\n",
    "    batchCount = int(pic.shape[0] / batchsize)\n",
    "    print('Epochs  ',epochs)\n",
    "    print('Batch_size   ',batchsize)\n",
    "    print('Batches per epoch    ',batchCount)\n",
    "    \n",
    "    pic2,target2=get_data('test')\n",
    "    \n",
    "    \n",
    "    generator = generator_model()\n",
    "    \n",
    "    discriminator = discriminator_model()\n",
    "    \n",
    "    gan = generator_containing_discriminator(generator,discriminator)\n",
    "    \n",
    "    generator.compile(loss=generator_l1_loss, optimizer='RMSprop')\n",
    "    \n",
    "    gan.compile(loss=[generator_l1_loss,discriminator_on_generator_loss] , optimizer='RMSprop')\n",
    "    \n",
    "    discriminator.trainable = True\n",
    "    \n",
    "    discriminator.compile(loss=discriminator_on_generator_loss, optimizer='RMSprop')\n",
    "    \n",
    "    \n",
    "    G_loss = []\n",
    "    D_loss = []\n",
    "    for e in range(1,epochs+1):\n",
    "        print('Epoch       {}',e)\n",
    "        for nums in range(batchCount):\n",
    "            random_number = np.random.randint(1,pic.shape[0],size=batchsize)\n",
    "            batch_pic = pic[random_number]\n",
    "            batch_target = target[random_number]\n",
    "            batch_target2 = np.tile(batch_target,(2,1,1,1))\n",
    "            y_dis = np.zeros((2*batchsize,30,30,1))\n",
    "            y_dis[:batchsize] = 1.0\n",
    "            \n",
    "            print('doing generator')\n",
    "            generated_pic = generator.predict(batch_target)\n",
    "            #Default is concat first dimention\n",
    "            concat_pic = np.concatenate((batch_pic,generated_pic))\n",
    "            \n",
    "            dis_input = np.concatenate((concat_pic,batch_target2),axis=-1)\n",
    "            \n",
    "            print('doing dicriminator')\n",
    "            dloss = discriminator.train_on_batch(dis_input,y_dis)\n",
    "            \n",
    "            random_number = np.random.randint(1,pic.shape[0],size=batchsize)\n",
    "            \n",
    "            train_target = target[random_number]\n",
    "            \n",
    "            batch_pic = pic[random_number]\n",
    "            \n",
    "            y_gener = np.ones((batchsize,30,30,1))\n",
    "            \n",
    "            discriminator.trainable = False\n",
    "            \n",
    "            print('doing combination')\n",
    "            gloss = gan.train_on_batch(train_target,[batch_pic,y_gener])\n",
    "            \n",
    "            discriminator.trainable = True\n",
    "            \n",
    "        paths = 'images'+str(e)\n",
    "        if os.path.isdir(paths)==False:\n",
    "            os.mkdir(paths)\n",
    "        for k in range(pic2.shape[0]):\n",
    "            filename='pic'+str(k)+'.png'\n",
    "            gg=os.path.join(paths, filename)\n",
    "            generate_pic(generator,pic2[k:k+1],gg)\n",
    "            \n",
    "        G_loss.append(gloss)\n",
    "        D_loss.append(dloss)\n",
    "        if e % epochs == 0:\n",
    "            generator.save('g%d.h5' % e)\n",
    "            \n",
    "            #discriminator.save('Model_para/pix2pix_d_epoch_%d.h5' % e)\n",
    "            \n",
    "            #gan.save('Model_para/pix2pix_gan_epoch_%d.h5' % e)\n",
    "            \n",
    "    D_loss = np.array(D_loss)\n",
    "    G_loss = np.array(G_loss)\n",
    "    np.save('Model_para/dloss.npy',D_loss)\n",
    "    np.save('Model_para/gloss.npy',G_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c507346ddca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
